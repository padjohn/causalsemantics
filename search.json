[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nCausal Semantics for everyone."
  },
  {
    "objectID": "extraction/annotation.html",
    "href": "extraction/annotation.html",
    "title": "Annotation Guidelines",
    "section": "",
    "text": "This page presents the annotation schema and guidelines used to produce the training data for C-BERT and the underlying (C, E, I) tuple representation.\nIf you are using C-BERT or working with annotated data from this project, this page serves as the reference for how annotations are structured, what decisions were made, and how edge cases are handled.\nThe annotation was developed over three iterations (2024‚Äì2025) and primarily conducted via INCEpTION. A total of 4,753 sentences from German environmental discourse (1990‚Äì2022) yielded 2,391 manually annotated causal relations.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#overview",
    "href": "extraction/annotation.html#overview",
    "title": "Annotation Guidelines",
    "section": "",
    "text": "This page presents the annotation schema and guidelines used to produce the training data for C-BERT and the underlying (C, E, I) tuple representation.\nIf you are using C-BERT or working with annotated data from this project, this page serves as the reference for how annotations are structured, what decisions were made, and how edge cases are handled.\nThe annotation was developed over three iterations (2024‚Äì2025) and primarily conducted via INCEpTION. A total of 4,753 sentences from German environmental discourse (1990‚Äì2022) yielded 2,391 manually annotated causal relations.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#annotation-schema",
    "href": "extraction/annotation.html#annotation-schema",
    "title": "Annotation Guidelines",
    "section": "Annotation Schema",
    "text": "Annotation Schema\nThe schema consists of two layers: span annotations mark causal components in text, and relation annotations link them into structured causal relations.\n\nSpans\nEach token or token sequence in a sentence receives at most one span label. There are two span categories.\nRole marks the primary causal components:\n\nIndicator ‚Äî the lexical marker that projects the causal relation\n\ne.g.¬†verursachen ‚Äòcause‚Äô, ist Ursache ‚Äòis cause‚Äô, weil ‚Äòbecause‚Äô, stoppen ‚Äòstop‚Äô\n\nEntity ‚Äî a cause or effect entity involved in the relation\n\ne.g.¬†Klimawandel ‚Äòclimate change‚Äô, Pestizide ‚Äòpesticides‚Äô\n\n\nCoefficient captures semantic modifiers:\n\nNegation\n\nNeutralizes or inverts the causal relation\nnicht ‚Äònot‚Äô, kein ‚Äòno‚Äô, Verlust ‚Äòloss‚Äô, Schwund ‚Äòshrinkage‚Äô\n\nDivision\n\nLowers salience\nauch ‚Äòalso‚Äô, unter anderem ‚Äòamong others‚Äô\n\nPriority\n\nIncreases salience\nvor allem ‚Äòespecially‚Äô, wichtig ‚Äòimportant‚Äô\n\n\nThe three types above directly impact influence. Other coefficients run orthogonal, as they either form constraints (e.g.¬†temporality) ‚Äì or specify epistemic dimensions (e.g Uncertainty).\n\n\n\n\n\n\nNoteOrthogonal coefficients\n\n\n\n\n\n\nObject-based constraints\n\n\n\n\n\n\n\nCoefficient\nFunction\nExamples\n\n\n\n\nTemporality\nTemporal framing\nseit, bereits, k√ºnftig, Jahre\n\n\nSpatiality\nSpatial framing\nglobal, lokal, weltweit, Deutschland\n\n\nQuality\nQualitative modification\nindustrielle, parasitischer, schwefelhaltiger\n\n\nQuantity\nQuantitative modification\ngro√ües, f√ºnftausend, zw√∂lf\n\n\n\n\nPropositional-based constraints\n\n\n\n\n\n\n\nCoefficient\nFunction\nExamples\n\n\n\n\nUncertainty\nModality and hedging\nm√∂glicherweise, k√∂nnte, vermutlich\n\n\nRepresentation\nEvidentiality marker\nlaut, sagt, sei\n\n\nRepresentation Entity\nSource/provenance\nStudie, Bericht, Forscher\n\n\n\n\n\n\n\n\nRelations\nAnnotated spans are linked by directed relations:\n\nCause: Indicator ‚Üí Entity (the entity is designated as a Cause)\nEffect: Indicator ‚Üí Entity (the entity fills the Effect role)\nConstraint: Entity or Indicator ‚Üí Coefficient (the coefficient modifies its governor)\n\n\n\nExample\nConsider the spans in this sentence:\n\n\n\n\nKlimawandel und insbesondere Landwirtschaft verst√§rken m√∂glicherweise das weltweite Artensterben.\n\n\nClimate change and especially agriculture possibly intensify global species extinction.\n\n\n\nTwo coordinated entities (Klimawandel und Landwirtschaft) are annotated separately with parallel Cause relations.\n The adverb insbesondere ‚Äòespecially‚Äô is scoped to Landwirtschaft, raising its salience as Priority. The sentence adverb m√∂glicherweise (Uncertainty) is parented to the indicator since it modifies the causal proposition.\n Lastly, the adjective weltweit is separately encoded as Spatiality coefficient ‚Äì in accordance with the token minimization principle.\n\n\n\n\n\n\n\n\ngraph LR\n    A[\"*verst√§rken*\"] --&gt;|Cause| B[\"*Klimawandel*\"]\n    A --&gt;|Cause| C[\"*Landwirtschaft*\"] --&gt;|Priority| D[\"*insbesondere*\"]\n    A --&gt;|Uncertainty| E[\"*m√∂glicherweise*\"]\n    A --&gt;|Effect| F[\"*Artensterben*\"]\n    F --&gt;|Spatiality| G[\"*weltweit*\"]",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#annotation-principles",
    "href": "extraction/annotation.html#annotation-principles",
    "title": "Annotation Guidelines",
    "section": "Annotation Principles",
    "text": "Annotation Principles\nFour principles guide annotation decisions in ambiguous cases.\n\nMinimal Principle\nOnly explicitly marked causal relations are annotated ‚Äî no inference. When a lexically specific marker (e.g.¬†verursacht) and a functional marker (e.g.¬†durch) co-occur, only the lexically richer element is annotated as indicator. Functional prepositions and connectors are annotated as indicators only when no richer causal lexeme is present.\nFor light verb constructions, only the semantically loaded element is annotated (e.g.¬†hat etwas mit X zu tun ‚Üí Indicator: tun).\n\n\nToken Minimization\nEntities are reduced to their head token. Attributive modifiers are extracted as separate coefficients:\n\nindustrielle Landwirtschaft ‚Üí Entity: Landwirtschaft, Coefficient: industrielle (Quality)\nEinsatz von Pestiziden ‚Üí Entity: Pestiziden\nException: named entities (Europ√§ische Union) and fixed multi-word expressions (saurer Regen) are annotated as single units.\n\nThis prevents proliferation of marginal entity variants and facilitates downstream aggregation.\n\n\nSyntactic Proximity\nWhen multiple potential entities compete for a role, the syntactically closest entity to the indicator is preferred, unless semantic considerations override this.\n\n\nCoefficient Conservatism\nCoefficients (other than Representation) are only annotated when they stand in a direct syntactic dependency relation with an indicator or entity.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#indicators",
    "href": "extraction/annotation.html#indicators",
    "title": "Annotation Guidelines",
    "section": "Indicators",
    "text": "Indicators\nIndicators are the lexical or syntactic markers that project causal relations and establish (C, E, I) tuples. They fulfill two functions: projecting causal roles onto syntactic positions in their co-text, and encoding inherent information about polarity and salience.\nThe annotation corpus contains 642 distinct indicator forms, grouped into 192 indicator families by morphological and semantic criteria. A family subsumes all realizations of a shared lexical core. E.g. the Ursache family includes, among other:\n\nverursachen ‚Äòcause‚Äô\nist Ursache ‚Äòis the cause‚Äô,\nUrsache sein ‚Äòto be the cause‚Äô\nTeilursache sein ‚Äòto be a partial cause‚Äô\n\n\nTop 10 Indicator Families\n\n\n\n\n\n\n\n\n\n\n\nFamily\nForms\nInstances\n\n\n\n\nUrsache  ‚ÄòCause‚Äô\n21\n167\n\n\nVerantwortung  ‚ÄòResponsibility‚Äô\n11\n122\n\n\nStoppen  ‚ÄòStop‚Äô\n3\n111\n\n\nGegen  ‚ÄòAgainst‚Äô\n3\n95\n\n\nDurch  ‚Äòthrough‚Äô\n2\n70\n\n\n\n\n\n\n\n\nFamily\nForms\nInstances\n\n\n\n\nBeitrag  ‚ÄòContribution‚Äô\n10\n70\n\n\nFolge  ‚ÄòConsequence‚Äô\n6\n69\n\n\nKampf  ‚ÄòFight‚Äô\n8\n69\n\n\nF√ºhren  ‚ÄòLead‚Äô\n9\n59\n\n\nGrund  ‚ÄòReason‚Äô\n7\n57\n\n\n\n\n\n\n\n\n\n\n\n\nNoteOrientation\n\n\n\n\n\nSome families are Cause-oriented, others are Effect-oriented. Compare:\n\nEmission_C is a cause of pollution_E.\nPollution_E is a consequence of emission_C.\n\nOrientation affects projection ‚Äì Cause-oriented indicators generally project Cause on the subject, Effect-oriented indicators do the same with Effect. Modified indicator forms affect the projection according to their orientation.\nNote the difference between Teilursache ‚Äòpartial cause‚Äô and Teilwirkung partial effect: The former implies the existence of several causes ‚Äì the latter the existence of several effects.\n\n\n\n\n\nPolarity and Salience\nEach indicator family carries a default polarity (promoting + or inhibiting -) and a default salience class:\n\nTable Caption\n\n\n\n\n\n\n\n\nFamily\nPolarity\nDefault Salience\nDiscourse Function\n\n\n\n\nUrsache  ‚ÄòCause‚Äô\n+\n[0.5-1]\nCause-oriented prototype\n\n\nFolge  ‚ÄòConsequence‚Äô\n+\n1\nEffect-oriented\n\n\nBeitrag  ‚ÄòContribution‚Äô\n+\n[0.5-0.75]\nDistributional attribution\n\n\nDurch  ‚ÄòThrough‚Äô\n+\n1\nGrammaticalized preposition\n\n\nStoppen  ‚ÄòStop‚Äô\n‚àí\n1\nIntervention framing\n\n\nGegen  ‚ÄòAgainst‚Äô\n‚àí\n1\nVariable condensation (Kampf/Protest gegen ‚Äòfight/protest against‚Äô)\n\n\nReduzieren  ‚ÄòReduce‚Äô\n‚àí\n[0.5-0.75]\nPrototypical distributional negative\n\n\nWirkung  ‚ÄòEffect‚Äô\n\\pm\n[0-1]\nWidest range of influence (Gegenwirkung ‚Äòcounter-effect‚Äô, wirkungslos ‚Äòineffective‚Äô )\n\n\n\n\n\nSyntactic Projection Patterns\nThe syntactic realization of each indicator determines how Cause and Effect roles are projected:\n\n\n\n\n\n\n\n\n\nFamily\nExample\nCause Projection\nEffect Projection\n\n\n\n\nUrsache  ‚ÄòCause‚Äô\nX ist Ursache f√ºr Y  ‚Äòx is the cause of‚Äô\nSubject\nPP (f√ºr)  ‚Äòfor‚Äô\n\n\nFolge  ‚ÄòConsequence‚Äô\nY ist Folge von X  ‚Äòx is a consequence of‚Äô\nPP (von)  ‚Äòof‚Äô\nSubject\n\n\nVerursachen  ‚ÄôCause\nX verursacht Y  ‚Äòx causes y‚Äô\nSubject\nAccusative object\n\n\nBeitragen  ‚ÄòContribution‚Äô\nX tr√§gt zu Y bei  ‚Äòx contributes to y‚Äô\nSubject\nPP (zu)  ‚Äòto‚Äô\n\n\nStoppen  ‚ÄòStop‚Äô\nX stoppt Y  ‚Äòx stops y‚Äô\nSubject\nAccusative object\n\n\nDurch  ‚ÄòThrough‚Äô\nY durch X  ‚Äòx through y‚Äô\nPP complement\nHead noun\n\n\nGegen  ‚ÄòAgainst‚Äô\nX gegen Y  ‚Äòx against y‚Äô\nSubject\nPP complement",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#context-markers",
    "href": "extraction/annotation.html#context-markers",
    "title": "Annotation Guidelines",
    "section": "Context Markers",
    "text": "Context Markers\nContext markers modify and contextualize the causal relations projected by indicators. Unlike indicators, they are not lexically specialized for causality but operate at the predication or proposition level. Three structural marker types directly affect the INFLUENCE computation:\n\nDivision\nDivision markers signal polycausal structures with implicit co-causes. Typical realizations include unter anderem ‚Äòamong other things‚Äô, auch ‚Äòalso‚Äô, ebenfalls ‚Äòlikewise‚Äô, and the composite nicht nur ‚Äònot only‚Äô.\nDivision markers reduce the salience to |I| = 0.5 regardless of the indicator‚Äôs default.\n\n\nPriority\nPriority markers establish asymmetric weighting within polycausal sets: vor allem ‚Äòabove all‚Äô, haupts√§chlich ‚Äòmainly, ma√ügeblich ‚Äôsignificantly‚Äô. They set |I| = 0.75 for the prioritized cause.\nBoth marker types affect only the salience (|I|), leaving polarity (\\pm) unchanged.\n\n\nNegation\nNegation is the structurally most influential marker. Two types are distinguished:\nObject-based negation operates at the entity level through markers like Verlust ‚Äòloss‚Äô, Schwund ‚Äòdecline‚Äô, R√ºckgang ‚Äòdecrease‚Äô. These invert the polarity when the count of negations is odd:\n\nVerlust von Lebensr√§umen verursacht Bienensterben.\nLoss of habitat causes bee decline.\nIndicator verursachen ‚Äòcause‚Äô: default I = 1;\nobject negation on Cause (Verlust ‚Äòloss‚Äô) inverts polarity: I = -1.\n\nPropositional negation operates at the relation level (nicht, kein) and neutralizes the entire causal relationship (I = 0):\n\nPestizide verursachen nicht Insektensterben.\nPesticides don‚Äôt cause Insektensterben.\nIndicator verursachen ‚Äòcause‚Äô: default I = 1\nPropositional negation nicht ‚Äòdon‚Äôt‚Äô neutralizes: I = 0.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#from-annotations-to-influence",
    "href": "extraction/annotation.html#from-annotations-to-influence",
    "title": "Annotation Guidelines",
    "section": "From Annotations to INFLUENCE",
    "text": "From Annotations to INFLUENCE\nThe annotations documented above ‚Äî indicators, entities, and context markers ‚Äî are the inputs to a deterministic computation that produces the final INFLUENCE value I \\in [-1, +1].\nIn brief: entity identification follows the indicator‚Äôs syntactic projection pattern, polarity is determined by indicator class and negation markers, and salience is computed through a cascading hierarchy of morphological, determiner, and syntactic markers.\nFor the full formal specification ‚Äî including the cascade rules, coordination normalization, and worked examples ‚Äî see Tuple Construction.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#data-format",
    "href": "extraction/annotation.html#data-format",
    "title": "Annotation Guidelines",
    "section": "Data Format",
    "text": "Data Format\nAnnotated data is exported as JSON. Each entry represents a sentence with its metadata and extracted relations.\n\nSchema\n{\n  \"subfolder\": \"Artensterben_oa\",\n  \"global_sentence_id\": 1148482,\n  \"text_id\": \"FAZ_200204_384209\",\n  \"text_date\": \"2002-04\",\n  \"sentence_id\": \"12\",\n  \"sentence\": \"...\",\n  \"relations\": [\n    {\n      \"indicator\": \"Folge\",\n      \"entities\": [\n        {\n          \"entity\": \"Kleinplanet\",\n          \"relation\": \"Cause\",\n          \"dependent_coefficients\": [\n            {\"coefficient_text\": \"Jahren\", \"coefficient\": \"Temporality\"},\n            {\"coefficient_text\": \"Mexikos\", \"coefficient\": \"Spatiality\"}\n          ]\n        },\n        {\n          \"entity\": \"Artensterben\",\n          \"relation\": \"Effect\"\n        }\n      ],\n      \"coefficient\": \"Division\",\n      \"dependent_coefficients\": [\n        {\"coefficient_text\": \"k√∂nnte\", \"coefficient\": \"Uncertainty\"}\n      ],\n      \"representation\": \"berieten\",\n      \"representation_entities\": [\n        {\n          \"entity\": \"Teilnehmer\",\n          \"relation\": \"Constraint\",\n          \"dependent_coefficients\": [\n            {\"coefficient_text\": \"f√ºnftausend\", \"coefficient\": \"Quantity\"}\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n\nField Reference\nSentence-level fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nsubfolder\nWABI subcorpus and syntactic position (e.g.¬†Artensterben_oa = accusative object)\n\n\nglobal_sentence_id\nUnique sentence identifier across the full corpus\n\n\ntext_id\nSource document identifier (format: SOURCE_YYYYMM_ID)\n\n\ntext_date\nPublication date (YYYY-MM)\n\n\nsentence\nFull sentence text\n\n\nrelations\nArray of causal relations found in this sentence (empty if none)\n\n\n\nRelation-level fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nindicator\nThe causal indicator lexeme\n\n\nentities\nArray of entities with their causal role (Cause or Effect)\n\n\ncoefficient\nStructural marker on the relation level (Negation, Division)\n\n\ndependent_coefficients\nContextual coefficients attached to the indicator\n\n\nrepresentation\nEvidentiality/speech-act verb (if present)\n\n\nrepresentation_entities\nSource entities for reported speech\n\n\n\nEntity-level fields:\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nentity\nHead token of the entity (token-minimized)\n\n\nrelation\nCausal role: Cause, Effect, or Constraint\n\n\ndependent_coefficients\nArray of coefficients modifying this entity\n\n\n\n\n\nSentences Without Relations\nSentences where no explicit causal relation was identified have an empty relations array. These are not noise ‚Äî they were reviewed during annotation and determined to contain no explicit causal markers per the minimal principle.",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#corpus-statistics",
    "href": "extraction/annotation.html#corpus-statistics",
    "title": "Annotation Guidelines",
    "section": "Corpus Statistics",
    "text": "Corpus Statistics\n\n\n\nTotal sentences\n4,753\n\n\nSentences with ‚â•1 WABI relation\n1,797 (37.8%)\n\n\nTotal WABI-relevant relations\n1,867\n\n\nDistinct indicator forms\n642\n\n\nIndicator families\n192\n\n\nMean relations per sentence\n0.39\n\n\n\nPer WABI term:\n\n\n\nTerm\nSentences\nRelations\nRel./Sent.\n\n\n\n\nWaldsterben\n1,818\n633\n0.35\n\n\nArtensterben\n1,854\n744\n0.40\n\n\nBienensterben\n536\n257\n0.48\n\n\nInsektensterben\n545\n233\n0.43",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "extraction/annotation.html#further-reading",
    "href": "extraction/annotation.html#further-reading",
    "title": "Annotation Guidelines",
    "section": "Further Reading",
    "text": "Further Reading\n\n\nFor how annotations are transformed into (C, E, I) tuples, see Tuple Construction\nFor the C-BERT model trained on this data, see C-BERT\nFull annotation data (Bundestag subset): HuggingFace Dataset",
    "crumbs": [
      "Extraction",
      "Annotation Guidelines"
    ]
  },
  {
    "objectID": "processing/processing.html",
    "href": "processing/processing.html",
    "title": "Processing Overview",
    "section": "",
    "text": "The processing module takes annotated causal relations ‚Äî whether produced manually or by C-BERT ‚Äî and transforms them into quantitative, aggregated representations suitable for corpus-level analysis.\n\n\n\n\n\ngraph LR\n    A[\"Annotated&lt;br/&gt;Relations&lt;br/&gt;(indicators, entities, markers)\"] --&gt; B[\"Tuple&lt;br/&gt;Construction\"]\n    B --&gt; C[\"Individual&lt;br/&gt;(C, E, I)&lt;br/&gt;Tuples\"]\n    C --&gt; D[\"Aggregation\"]\n    D --&gt; E[\"Normalized&lt;br/&gt;Causal&lt;br/&gt;Patterns\"]\n    E --&gt; F[\"Focus-Term&lt;br/&gt;Analysis\"]\n    E --&gt; G[\"ACG&lt;br/&gt;Networks\"]\n\n\n\n\n\n\nThis transformation happens in two stages:",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/processing.html#overview",
    "href": "processing/processing.html#overview",
    "title": "Processing Overview",
    "section": "",
    "text": "The processing module takes annotated causal relations ‚Äî whether produced manually or by C-BERT ‚Äî and transforms them into quantitative, aggregated representations suitable for corpus-level analysis.\n\n\n\n\n\ngraph LR\n    A[\"Annotated&lt;br/&gt;Relations&lt;br/&gt;(indicators, entities, markers)\"] --&gt; B[\"Tuple&lt;br/&gt;Construction\"]\n    B --&gt; C[\"Individual&lt;br/&gt;(C, E, I)&lt;br/&gt;Tuples\"]\n    C --&gt; D[\"Aggregation\"]\n    D --&gt; E[\"Normalized&lt;br/&gt;Causal&lt;br/&gt;Patterns\"]\n    E --&gt; F[\"Focus-Term&lt;br/&gt;Analysis\"]\n    E --&gt; G[\"ACG&lt;br/&gt;Networks\"]\n\n\n\n\n\n\nThis transformation happens in two stages:",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/processing.html#tuple-construction",
    "href": "processing/processing.html#tuple-construction",
    "title": "Processing Overview",
    "section": "Tuple Construction",
    "text": "Tuple Construction\nIndividual annotated relations are converted into formal (C, E, I) tuples through a deterministic three-step algorithm. Entity identification uses syntactic projection patterns to extract Cause and Effect from the indicator‚Äôs argument structure.\nPolarity determination computes the sign of I from the indicator‚Äôs inherent class and any negation markers.\nSalience calculation computes the magnitude |I| through a cascading hierarchy of morphological, determiner, and syntactic markers. The output is a fully specified tuple where I = \\pm(\\text{polarity}) \\times |\\text{salience}| \\in [-1, +1].\n‚Üí Tuple Construction: Full algorithm with cascade rules, coordination normalization, and worked examples",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/processing.html#aggregation",
    "href": "processing/processing.html#aggregation",
    "title": "Processing Overview",
    "section": "Aggregation",
    "text": "Aggregation\nIndividual tuples are condensed into cumulative causal patterns through weighted summation and normalization. Identical tuples are counted; tuples sharing the same (C, E) pair are summed (with frequency √ó salience weighting); and the aggregated values are normalized to produce proportional influence scores. Two normalization strategies serve different analysis goals: bidirectional normalization for exhaustive focus-term analysis, and unidirectional normalization for full causal graph construction.\n‚Üí Aggregation: Full pipeline with normalization formulas, polarity handling, and the resulting graph data structure",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/processing.html#design-principles",
    "href": "processing/processing.html#design-principles",
    "title": "Processing Overview",
    "section": "Design Principles",
    "text": "Design Principles\nCompositionality. Aggregation takes tuple values as given ‚Äî any refinement to the tuple construction rules flows directly into the aggregated output without requiring changes to the aggregation pipeline.\nSeparation of concerns. Tuple construction is a linguistic operation (mapping annotations to formal values); aggregation is a statistical operation (condensing evidence across attestations). The two are cleanly decoupled.\nMetadata preservation. Each tuple carries source metadata (text ID, date, contextual markers). These enable differential analyses ‚Äî temporal stratification, source-specific filtering ‚Äî but do not enter the core aggregation computation.",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/processing.html#continue",
    "href": "processing/processing.html#continue",
    "title": "Processing Overview",
    "section": "Continue",
    "text": "Continue\n\nTuple Construction ‚Äî the formal algorithm for computing (C, E, I) values\nAggregation ‚Äî weighting, summation, and normalization across attestations\nBack to Extraction ‚Äî how annotations are produced",
    "crumbs": [
      "Processing",
      "Processing Overview"
    ]
  },
  {
    "objectID": "processing/aggregation.html",
    "href": "processing/aggregation.html",
    "title": "Aggregation",
    "section": "",
    "text": "While tuple construction formalizes individual attestations, aggregation addresses the scaling problem: how do hundreds or thousands of individual (C, E, I) tuples ‚Äî extracted from different texts, time periods, and discursive contexts ‚Äî condense into representative causal patterns?\nThe aggregation pipeline transforms a set of individual tuples into normalized, proportional causal weights through four steps: counting identical tuples, weighting by frequency and salience, summing across attestations for each (C, E) pair, and normalizing to produce proportional influence scores.\n\n\n\n\n\ngraph LR\n    A[\"Individual&lt;br/&gt;(C, E, I) Tuples\"] --&gt; B[\"Count&lt;br/&gt;Identical Tuples\"]\n    B --&gt; C[\"Weight&lt;br/&gt;(frequency √ó salience)\"]\n    C --&gt; D[\"Sum per&lt;br/&gt;(C, E) Pair\"]\n    D --&gt; E[\"Normalize\"]\n    E --&gt; F[\"Proportional&lt;br/&gt;Influence Scores\"]",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#overview",
    "href": "processing/aggregation.html#overview",
    "title": "Aggregation",
    "section": "",
    "text": "While tuple construction formalizes individual attestations, aggregation addresses the scaling problem: how do hundreds or thousands of individual (C, E, I) tuples ‚Äî extracted from different texts, time periods, and discursive contexts ‚Äî condense into representative causal patterns?\nThe aggregation pipeline transforms a set of individual tuples into normalized, proportional causal weights through four steps: counting identical tuples, weighting by frequency and salience, summing across attestations for each (C, E) pair, and normalizing to produce proportional influence scores.\n\n\n\n\n\ngraph LR\n    A[\"Individual&lt;br/&gt;(C, E, I) Tuples\"] --&gt; B[\"Count&lt;br/&gt;Identical Tuples\"]\n    B --&gt; C[\"Weight&lt;br/&gt;(frequency √ó salience)\"]\n    C --&gt; D[\"Sum per&lt;br/&gt;(C, E) Pair\"]\n    D --&gt; E[\"Normalize\"]\n    E --&gt; F[\"Proportional&lt;br/&gt;Influence Scores\"]",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#step-1-counting-identical-tuples",
    "href": "processing/aggregation.html#step-1-counting-identical-tuples",
    "title": "Aggregation",
    "section": "Step 1: Counting Identical Tuples",
    "text": "Step 1: Counting Identical Tuples\nTuples with identical (C, E, I) values are grouped and counted. The frequency n quantifies how often a specific tuple configuration occurs in the corpus.\n\n\n\n\n\n\nNoteExample\n\n\n\nInput (5 individual tuples):\n\n\n\nC\nE\nI\n\n\n\n\nPestizide\nInsektensterben\n+1.0\n\n\nPestizide\nInsektensterben\n+0.5\n\n\nPestizide\nInsektensterben\n+0.5\n\n\nKlimawandel\nInsektensterben\n+0.5\n\n\nPestizidverbote\nInsektensterben\n‚àí0.5\n\n\n\nOutput (4 weighted tuples):\n\n\n\nC\nE\nI\nn\n\n\n\n\nPestizide\nInsektensterben\n+1.0\n1\n\n\nPestizide\nInsektensterben\n+0.5\n2\n\n\nKlimawandel\nInsektensterben\n+0.5\n1\n\n\nPestizidverbote\nInsektensterben\n‚àí0.5\n1\n\n\n\n\n\nThis distinction matters: ten attestations of monocausal attribution (Pestizide, Insektensterben, +1.0) carry ten times the weight of a single attestation of polycausal attribution (Pestizide, Insektensterben, +0.5).",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#step-2-weighted-summation",
    "href": "processing/aggregation.html#step-2-weighted-summation",
    "title": "Aggregation",
    "section": "Step 2: Weighted Summation",
    "text": "Step 2: Weighted Summation\nTuples sharing the same (C, E) pair but differing in I values are summed into a single aggregated relation. The aggregated influence is:\n\nF_{C,E} = \\sum_{i} I_i \\times n_i\n\nwhere I_i are the individual INFLUENCE values and n_i their frequencies. Polarity-specific counters are tracked separately to capture discursive disagreement.\n\n\n\n\n\n\nNoteExample (continuing from above)\n\n\n\nFor the Pestizide ‚Üí Insektensterben pair:\nF = (1.0 \\times 1) + (0.5 \\times 2) = +2.0\nPolarity counters: n_{\\text{pos}} = 3, n_{\\text{neg}} = 0, n_{\\text{neutral}} = 0\nFull output:\n\n\n\n\n\n\n\n\n\n\n\nC\nE\nF_{\\text{agg}}\nn_{\\text{pos}}\nn_{\\text{neg}}\nn_{\\text{neutral}}\n\n\n\n\nPestizide\nInsektensterben\n+2.0\n3\n0\n0\n\n\nKlimawandel\nInsektensterben\n+0.5\n1\n0\n0\n\n\nPestizidverbote\nInsektensterben\n‚àí0.5\n0\n1\n0\n\n\n\n\n\nThree properties of this summation are worth noting. Neutralized relations (I = 0, from propositional negation) contribute zero to F_{C,E} but are counted in n_{\\text{neutral}} to document denied causal claims. Opposing polarities partially cancel: if the same entity is attributed as both promoting and inhibiting a given effect (e.g.¬†through contradicting sources or temporal shifts), the aggregated value reflects the net balance, while the polarity counters (n_{\\text{pos}} &gt; 0 and n_{\\text{neg}} &gt; 0 simultaneously) expose the controversy. Salience is already encoded in the I values from tuple construction ‚Äî a monocausal attestation (I = 1.0) contributes twice the weight of a distributed attestation (I = 0.5), so frequency and salience interact multiplicatively.",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#step-3-normalization",
    "href": "processing/aggregation.html#step-3-normalization",
    "title": "Aggregation",
    "section": "Step 3: Normalization",
    "text": "Step 3: Normalization\nThe aggregated values F_{C,E} are normalized to produce proportional influence scores I_{\\text{norm}} \\in [-1, +1], where the sum of absolute values across all co-relations equals approximately 1.0. The normalization strategy depends on the analysis context.\n\nBidirectional Normalization (Focus-Term Analysis)\nWhen analyzing a specific term T exhaustively ‚Äî examining all its incoming causes and outgoing effects ‚Äî both directions are normalized independently:\n\nI_{\\text{norm}}(C \\to T) = \\text{sgn}(F_{C,T}) \\times \\frac{|F_{C,T}|}{\\sum_{C' \\in \\text{Causes}(T)} |F_{C',T}|}\n\n\nI_{\\text{norm}}(T \\to E) = \\text{sgn}(F_{T,E}) \\times \\frac{|F_{T,E}|}{\\sum_{E' \\in \\text{Effects}(T)} |F_{T,E'}|}\n\nThis is appropriate when the annotation exhaustively covers all relations involving a focal term but does not cover the co-causes of its effects (e.g.¬†all causes of Insektensterben are annotated, but not all causes of Klimawandel).\n\n\nUnidirectional Normalization (ACG Networks)\nFor full causal graph construction, only cause-side normalization is applied ‚Äî the standard asymmetry of causal graphs:\n\nI_{\\text{norm}}(C \\to E) = \\text{sgn}(F_{C,E}) \\times \\frac{|F_{C,E}|}{\\sum_{C' \\in \\text{Causes}(E)} |F_{C',E}|}\n\nThis ensures that, for any effect E, the absolute influence values of all its causes sum to 1.0.\n\n\n\n\n\n\nNoteNormalization Example (unidirectional)\n\n\n\nInput (from Step 2):\n\n\n\nC ‚Üí E\nF_{\\text{agg}}\n\n\n\n\nPestizide ‚Üí Insektensterben\n+2.0\n\n\nKlimawandel ‚Üí Insektensterben\n+0.5\n\n\nPestizidverbote ‚Üí Insektensterben\n‚àí0.5\n\n\n\nDenominator: |2.0| + |0.5| + |0.5| = 3.0\nOutput:\n\n\n\n\n\n\n\n\nC ‚Üí E\nI_{\\text{norm}}\nInterpretation\n\n\n\n\nPestizide ‚Üí Insektensterben\n+0.667\n66.7% of causal attribution (promoting)\n\n\nKlimawandel ‚Üí Insektensterben\n+0.167\n16.7% (promoting)\n\n\nPestizidverbote ‚Üí Insektensterben\n‚àí0.167\n16.7% (inhibiting)\n\n\n\nSum of absolute values: 0.667 + 0.167 + 0.167 = 1.0 ‚úì\n\n\nNormalization operates on absolute values but preserves the sign via the \\text{sgn} function. Promoting and inhibiting relations are normalized jointly ‚Äî the sign is re-applied after normalization. The polarity-specific counters (n_{\\text{pos}}, n_{\\text{neg}}, n_{\\text{neutral}}) remain unchanged, since normalization scales only the weights, not the underlying evidence counts.",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#step-4-structuring",
    "href": "processing/aggregation.html#step-4-structuring",
    "title": "Aggregation",
    "section": "Step 4: Structuring",
    "text": "Step 4: Structuring\nThe normalized relations are stored as a directed graph where each edge (C \\to E) carries:\n\n\n\n\n\n\n\nAttribute\nDescription\n\n\n\n\ninfluence_norm\nNormalized influence I \\in [-1, 1]\n\n\ntuple_count\nTotal underlying tuples (n_{\\text{pos}} + n_{\\text{neg}} + n_{\\text{neutral}})\n\n\ncount_pos\nAttestations with I &gt; 0\n\n\ncount_neg\nAttestations with I &lt; 0\n\n\ncount_neutral\nAttestations with I = 0 (propositional negation)\n\n\n\nThis structure supports two complementary operations: local entity extraction ‚Äî retrieving all causes and effects of a specific entity for focused analysis ‚Äî and global centrality measures ‚Äî comparing the structural role of all entities in the causal discourse network.",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#analysis-modes",
    "href": "processing/aggregation.html#analysis-modes",
    "title": "Aggregation",
    "section": "Analysis Modes",
    "text": "Analysis Modes\nThe aggregated graph feeds two analysis modes, each with its own normalization strategy:\nFocus-Term Analysis positions a single term as a causal nucleus and examines its incoming causes and outgoing effects with bidirectional normalization. Each causal interactant is characterized by three metrics: normalized influence (I\\%), mean pre-aggregation salience (\\varnothing|I|, indicating whether the interactant is typically framed monocausally or polycausally), and a Gini coefficient measuring concentration of influence across all interactants (0 = evenly distributed, 1 = fully concentrated on one entity).\nACG Construction treats all entities as nodes in a directed graph with unidirectional normalization, enabling network-level analysis: centrality, community detection, and structural comparison across time periods or corpora.",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#compositionality",
    "href": "processing/aggregation.html#compositionality",
    "title": "Aggregation",
    "section": "Compositionality",
    "text": "Compositionality\nA key design principle is that aggregation is compositional: it takes the tuple values from tuple construction as given. Any refinement to the tuple construction rules (e.g.¬†finer-grained salience computation) flows directly into aggregation without requiring changes to the aggregation pipeline itself. The choice of normalization strategy and the handling of opposing polarities are analytical decisions that depend on the research context.",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "processing/aggregation.html#further-reading",
    "href": "processing/aggregation.html#further-reading",
    "title": "Aggregation",
    "section": "Further Reading",
    "text": "Further Reading\n\nFor how individual tuples are computed from annotations, see Tuple Construction\nFor the annotation schema that produces the inputs, see Annotation Guidelines",
    "crumbs": [
      "Processing",
      "Aggregation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Semantics (S_C)",
    "section": "",
    "text": "Causal Semantics is a computational framework for extracting and analyzing causal relations from natural language text. It bridges three research traditions:\n\nLogical theories differentiating between monocausal and polycausal structures\nLinguistic analyses identifying semantic dimensions like promoting vs.¬†inhibiting influences\nComputational methods that scale, but tend to reduce relations to binary cause-effect pairs\n\nIn short, S_C aims to extract and represent causal attributions:\n\nClimate change causes species extinction.\n\nand represent them as graphs:\n \\text{Climate change} \\xrightarrow{+1} \\text{Species extinction}"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Causal Semantics (S_C)",
    "section": "",
    "text": "Causal Semantics is a computational framework for extracting and analyzing causal relations from natural language text. It bridges three research traditions:\n\nLogical theories differentiating between monocausal and polycausal structures\nLinguistic analyses identifying semantic dimensions like promoting vs.¬†inhibiting influences\nComputational methods that scale, but tend to reduce relations to binary cause-effect pairs\n\nIn short, S_C aims to extract and represent causal attributions:\n\nClimate change causes species extinction.\n\nand represent them as graphs:\n \\text{Climate change} \\xrightarrow{+1} \\text{Species extinction}"
  },
  {
    "objectID": "index.html#core-innovation",
    "href": "index.html#core-innovation",
    "title": "Causal Semantics (S_C)",
    "section": "Core Innovation",
    "text": "Core Innovation\nThe framework models causal relations as (C, E, I) tuples, where:\n\nC (Cause): The causing entity\nE (Effect): The affected entity\n\nI (Influence): A signed scalar \\in [-1, +1] encoding:\n\nPolarity (sign): Promoting (+) vs.¬†inhibiting (‚àí) influence\nSalience (magnitude): Monocausal (|I|=1.0) vs.¬†polycausal (|I|&lt;1.0) attribution\n\n\nThis representation enables:\n\nSemantic precision: Capturing the direction and strength of a causal attribution\nQuantitative aggregation: Accumulating attributions into weighted causal networks\nGraph-based analysis: Visualizing discourse dynamics as Attributional Causal Graphs (ACGs)"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "Causal Semantics (S_C)",
    "section": "Example",
    "text": "Example\nConsider these sentences from environmental discourse:\n\nClimate change causes species extinction.\n(C_\\text{climate change}, E_\\text{species extinction}, I_{+1.0})\n\n\nConservation measures reduce forest dieback.\n(C_\\text{conservation measures}, E_\\text{forest dieback}, I_{-0.5})\n\nThe first example expresses a promoting, monocausal relation (I=+1.0), while the second expresses an inhibiting, contributory relation (I=-0.5)."
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "Causal Semantics (S_C)",
    "section": "Architecture",
    "text": "Architecture\nThe framework consists of three main modules:\n\n\n\n\n\ngraph LR\n    A[Text Input] --&gt; B1[Annotation]\n    A[Text Input] --&gt; B2[C-BERT]\n\n    B1 --&gt; C[Tuple-Construction]\n    B2 --&gt; C\n\n    C --&gt; D[Aggregation]\n    D --&gt; E[ACG]\n    \n    E --&gt; D2[Visualization]\n    E --&gt; D3[Analysis]\n\n\n\n\n\n\n\nExtraction: Identifying causal relations in text through indicators, annotation schemes, and the C-BERT transformer\nProcessing: Converting annotations into formal (C,E,I) tuples and aggregating them"
  },
  {
    "objectID": "index.html#applications",
    "href": "index.html#applications",
    "title": "Causal Semantics (S_C)",
    "section": "Applications",
    "text": "Applications\nThis framework has been applied to a German Environmental corpus (1990-2022) to:\n\nanalyze responsibility attributions in biodiversity debates [1]\ndisambiguate references to forest diebacks [2]"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Causal Semantics (S_C)",
    "section": "Citation",
    "text": "Citation\nIf you use this framework in your research, please cite:\n@phdthesis{johnson2026causalsemantics,\n  title={Kausalsemantik. Eine Operationalisierung der -sterben Komposita im Umweltdiskurs},\n  author={Patrick Johnson},\n  school={Technical University of Darmstadt},\n  year={forthcoming}\n}\n@misc{cbert,\n  title={C-BERT: Factorized Causal Relation Extraction},\n  author={Patrick Johnson},\n  doi={10.26083/tuda-7797},\n  year={2026}\n}"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Noteü§ñ C-BERT Model\n\n\n\npadjohn/cbert\nLibrary to create datasets, train and run the Multi-task transformer for causal relation extraction.\n\n\n\n\n\n\n\n\n\n\n\nNoteü§ó Hugging Face\n\n\n\npdjohn/c-bert\nPre-trained C-BERT model weights for German causal extraction."
  },
  {
    "objectID": "resources.html#code-models",
    "href": "resources.html#code-models",
    "title": "Resources",
    "section": "",
    "text": "Noteü§ñ C-BERT Model\n\n\n\npadjohn/cbert\nLibrary to create datasets, train and run the Multi-task transformer for causal relation extraction.\n\n\n\n\n\n\n\n\n\n\n\nNoteü§ó Hugging Face\n\n\n\npdjohn/c-bert\nPre-trained C-BERT model weights for German causal extraction."
  },
  {
    "objectID": "resources.html#publications",
    "href": "resources.html#publications",
    "title": "Resources",
    "section": "Publications",
    "text": "Publications\n\nThesis\n\n\n\n\n\n\nTipüìñ Dissertation\n\n\n\nTitle: Kausalsemantik. Eine Operationalisierung der -sterben Komposita im Umweltdiskurs\nAuthor: Patrick\nInstitution: Technical University Darmstadt\nYear: 2026\nLink: [coming soon]\nThe thesis introduces the Causal Semantics framework and applies it to analyze responsibility attributions in German environmental discourse (1990-2020).\n\n\n\n\nPapers\n\n\n\n\n\n\nTipüìÑ C-BERT Paper\n\n\n\nTitle: C-BERT: Factorized Causal Relation Extraction\nAuthors: Patrick Johnson\nYear: 2026\nLink: DOI | PDF\nIntroduces the C-BERT multi-task transformer for extracting (C,E,I) tuples from German text."
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Resources",
    "section": "Data",
    "text": "Data\n\nIndicator Library\nThe framework uses a library of 644 German causal indicators across multiple families:\n\nüî¢ Size: 644 indicator forms\nüè∑Ô∏è Families: 162 semantic families (CAUSE, STOP, THROUGH, etc.)\nüìä Distribution: Annotated with frequency and priority\nüß≤ Polarity: Each indicator marked as promoting (+) or inhibiting (‚àí)\nüåü Salience: Each indicator marked as mono, distributive (0.5) or priority (‚àí)\n\nDownload: indicators.csv\n\n\nAnnotation Corpus\nManual annotations of causal relations in German environmental texts:\n\nüìÑ Size: 2,391 annotated relations\nüìÖ Period: 1990-2020\nüåç Domain: Environmental discourse (forest dieback, species extinction, insect mortality, bee mortality)\nüè∑Ô∏è Annotations: Indicators, entities, polarity, salience, context markers\n\nAvailability: A subset containing the sentences from the German Bundestag is available on Huggingface."
  },
  {
    "objectID": "resources.html#related-work",
    "href": "resources.html#related-work",
    "title": "Resources",
    "section": "Related Work",
    "text": "Related Work\n\nCausal Extraction Systems\n\nBECauSE [1]: Corpus of causal and purpose relations\n\n\n\nLinguistic Frameworks\n\nCruse [2]: Lexical semantic analysis of causative verbs\nWolff [3]: Force dynamics in causal reasoning\nTalmy [4]: Force dynamics in language and cognition\n\n\n\nComputational Methods\n\nBERT-based Extraction [5]: Transformer models for causal relation classification\nKnowledge Graphs [6]: Event-Influence Extraction"
  },
  {
    "objectID": "resources.html#tools-libraries",
    "href": "resources.html#tools-libraries",
    "title": "Resources",
    "section": "Tools & Libraries",
    "text": "Tools & Libraries\nThe framework builds on several open-source tools:\n\nHugging Face Transformers: Base models and training infrastructure\nspaCy: Dependency parsing for syntactic projection\nNetworkX: Graph construction and analysis\nPlotly: Interactive visualizations"
  },
  {
    "objectID": "resources.html#contact-collaboration",
    "href": "resources.html#contact-collaboration",
    "title": "Resources",
    "section": "Contact & Collaboration",
    "text": "Contact & Collaboration\nInterested in using or extending this framework? We welcome collaborations!\nüìß Email, üü¢ ORCiD, üíº LinkedIn"
  },
  {
    "objectID": "resources.html#citation",
    "href": "resources.html#citation",
    "title": "Resources",
    "section": "Citation",
    "text": "Citation\nIf you use this framework in your research, please cite:\n@phdthesis{johnson2026causalsemantics,\n  title={Kausalsemantik. Eine Operationalisierung der -sterben Komposita im Umweltdiskurs},\n  author={Patrick Johnson},\n  school={Technical University of Darmstadt},\n  year={forthcoming}\n}\n@misc{cbert,\n  title={C-BERT: Factorized Causal Relation Extraction},\n  author={Patrick Johnson},\n  doi={10.26083/tuda-7797},\n  year={2026}\n}"
  },
  {
    "objectID": "resources.html#license",
    "href": "resources.html#license",
    "title": "Resources",
    "section": "License",
    "text": "License\nThe code is released under the MIT License.\nThe documentation is licensed under CC BY 4.0."
  },
  {
    "objectID": "resources.html#acknowledgments",
    "href": "resources.html#acknowledgments",
    "title": "Resources",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis research was funded by the Deutsche Forschungsgemeinschaft (DFG) as part of the project FOR 5182 / ‚ÄúKontroverse Diskurse. Sprachgeschichte als Zeitgeschichte seit 1990‚Äù. The author is deeply grateful for the financial support that made this work possible. I also thank the members of the research group for their invaluable feedback and stimulating discussions throughout the development of the Causal Semantics framework."
  },
  {
    "objectID": "processing/tuple-construction.html",
    "href": "processing/tuple-construction.html",
    "title": "Tuple Construction",
    "section": "",
    "text": "Tuple construction transforms qualitative linguistic annotations (indicators, markers) into quantitative (C, E, I) values through a systematic three-step process:\n\nEntity identification: Extract C and E based on syntactic projection\nPolarity determination: Calculate the sign of I from indicator class and negation\nSalience calculation: Calculate the magnitude |I| from morphological and syntactic markers\n\nThe output is a fully specified triple (C, E, I) where I \\in [-1, +1] represents:\n\nI = \\pm(\\text{polarity}) \\times |\\text{salience}|",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#overview",
    "href": "processing/tuple-construction.html#overview",
    "title": "Tuple Construction",
    "section": "",
    "text": "Tuple construction transforms qualitative linguistic annotations (indicators, markers) into quantitative (C, E, I) values through a systematic three-step process:\n\nEntity identification: Extract C and E based on syntactic projection\nPolarity determination: Calculate the sign of I from indicator class and negation\nSalience calculation: Calculate the magnitude |I| from morphological and syntactic markers\n\nThe output is a fully specified triple (C, E, I) where I \\in [-1, +1] represents:\n\nI = \\pm(\\text{polarity}) \\times |\\text{salience}|",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#step-1-entity-identification",
    "href": "processing/tuple-construction.html#step-1-entity-identification",
    "title": "Tuple Construction",
    "section": "Step 1: Entity Identification",
    "text": "Step 1: Entity Identification\nInput: Annotated causal relation with indicator and syntactic dependencies\nOutput: Entity pair (C, E)\nMethod: Syntactic projection according to indicator-specific patterns\nCausal indicators project their arguments through predictable syntactic patterns. The most frequent patterns:\n\nTransitive-Causative Verbs\nIndicators: cause, trigger, produce, stop, prevent\nProjection: Subject ‚Üí Cause, Direct object ‚Üí Effect\n\n\n\n\n\n\nNoteExample\n\n\n\nPesticides cause insect mortality.\n\nIndicator: cause (transitive-causative)\nSubject (pesticides) = Cause\nDirect object (insect mortality) = Effect\nResult: (C=\\text{pesticides}, E=\\text{insect mortality})\n\n\n\n\n\nCopula Constructions\nIndicators: cause (noun), consequence, reason\nProjection: Subject ‚Üí Cause, Prepositional object (for/of) ‚Üí Effect\n\n\n\n\n\n\nNoteExample\n\n\n\nClimate change is the cause of species extinction.\n\nIndicator: cause (copula construction)\nSubject (climate change) = Cause\n\nPrepositional object (of species extinction) = EFFECT\nResult: (C=\\text{climate change}, E=\\text{species extinction})\n\n\n\n\n\nPrepositional Markers\nIndicators: due to, because of, through\nProjection: Prepositional object ‚Üí Cause, Matrix clause subject/object ‚Üí Effect\n\n\n\n\n\n\nNoteExample\n\n\n\nSpecies die out due to habitat loss.\n\nIndicator: due to (prepositional)\nPrepositional object (habitat loss) = Cause\nMatrix subject (species) combined with verb (die out) = Effect\nResult: (C=\\text{habitat loss}, E=\\text{species die out})\n\n\n\n\n\nEntity Minimization\nExtracted entities follow the token minimization principle: attributive modifiers are extracted as separate coefficients, leaving only head tokens as entities.\n\n\n\n\n\n\nTipWhy minimization?\n\n\n\nMinimal entities enable better aggregation. Instead of treating ‚Äúindustrial pesticides‚Äù and ‚Äúagricultural pesticides‚Äù as separate causes, we extract:\n\nEntity: pesticides\nCoefficient: industrial / agricultural\n\nThis allows aggregating evidence about pesticides as a general cause while preserving modifier information for detailed analysis.",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#step-2-polarity-determination",
    "href": "processing/tuple-construction.html#step-2-polarity-determination",
    "title": "Tuple Construction",
    "section": "Step 2: Polarity Determination",
    "text": "Step 2: Polarity Determination\nInput: Entity pair (C, E) and annotated indicator with optional negation markers\nOutput: Sign of I (+ or ‚àí)\nMethod: Base polarity from indicator class, modified by negation\n\nBase Polarity from Indicator Class\nEach indicator family has an inherent polarity:\nPromoting indicators (I_{\\text{default}} &gt; 0): - Verbs: cause, trigger, lead to, produce, strengthen - Nouns: cause, reason, consequence - Prepositions: due to, because of, through\nInhibiting indicators (I_{\\text{default}} &lt; 0): - Verbs: stop, prevent, reduce, block, curb - Nouns: prevention, barrier, protection against - Prepositions: against, despite\n\n\n\n\n\n\nNoteExample\n\n\n\nMeasures stop insect mortality.\n\nIndicator: stop ‚àà STOP family (inhibiting)\nBase polarity: I_{\\text{default}} &lt; 0\n\n\n\n\n\nNegation Modification\nContextual negation markers modify base polarity through two mechanisms:\n\nObject-Based Negation\nNegative nominals (loss, decline, absence) invert polarity with odd numbers of negations:\n\n\\begin{align*}\n\\text{1 negation:} \\quad &I_{\\text{final}} = -I_{\\text{default}} \\\\\n\\text{2 negations:} \\quad &I_{\\text{final}} = I_{\\text{default}} \\\\\n\\text{3 negations:} \\quad &I_{\\text{final}} = -I_{\\text{default}}\n\\end{align*}\n\n\n\n\n\n\n\nNoteExample: Single negation\n\n\n\nLoss of habitats causes bee mortality.\n\nIndicator: causes ‚Üí I_{\\text{default}} &gt; 0 (promoting)\nObject negation on Cause (loss): 1√ó\nPolarity inverted: I_{\\text{final}} &lt; 0 (inhibiting)\nInterpretation: Less habitat leads to more bee mortality (inhibiting relation)\n\n\n\n\n\n\n\n\n\nNoteExample: Double negation\n\n\n\nLoss of pesticides prevents loss of bees.\n\nIndicator: prevents ‚Üí I_{\\text{default}} &lt; 0 (inhibiting)\nObject negations: loss Cause + loss (Effect) = 2√ó\nPolarity preserved: I_{\\text{final}} &lt; 0 (inhibiting)\nInterpretation: Less pesticides leads to fewer bee deaths (inhibiting relation)\n\n\n\n\n\nPropositional Negation\nPropositional negation (not cause, doesn‚Äôt prevent) neutralizes the relation:\n\n\n\n\n\n\nNoteExample\n\n\n\nPesticides do not cause bee mortality.\n\nIndicator: cause ‚Üí I_{\\text{default}} &gt; 1\nVerbal negation: not\nInfluence neutralized: I_{\\text{final}} = 0\n\n\n\n\n\n\n\n\n\nWarningComplex Negation\n\n\n\nThe framework currently doesn‚Äôt differentiate between neutralized positive (e.g.¬†not causing) and neutralized negative (e.g.¬†not preventing) relationships, as both result in 0.",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#step-3-salience-calculation",
    "href": "processing/tuple-construction.html#step-3-salience-calculation",
    "title": "Tuple Construction",
    "section": "Step 3: Salience Calculation",
    "text": "Step 3: Salience Calculation\nInput: Entity pair (C, E) with annotated markers\nOutput: Magnitude |I| \\in [0,1]\nMethod: Combine explicit markers and structural distribution\nSalience emerges from two factors:\n\nExplicit Lexical Markers\nMarkers directly specify relative weight:\nMonocausal (|I| = 1.0): - Determination: the cause (not a cause) - Exclusivity: responsible for, the reason - No competing causes mentioned\nPrioritized (|I| = 0.75): - Emphasis: mainly, primarily, above all - Composition: main cause, key factor\nDistributed (|I| = 0.5): - Contribution: contributes to, plays a role - Composition: partial cause, one factor - Distribution: among other things, also\n\n\nStructural Distribution\nMultiple coordinated causes distribute salience proportionally:\n\n\n\nConstruction\nEach cause gets\n\n\n\n\nX causes Z (alone)\n\\|I\\| = 1.0\n\n\nX and Y cause Z\n\\|I\\| = 0.5\n\n\nA, B, and C cause Z\n\\|I\\| = 0.33\n\n\n\n\n\n\n\n\n\nNoteExample: Explicit + Structural\n\n\n\nX and Y are two main causes of Z.\n\nExplicit marker: main causes ‚Üí base salience = 0.75\nStructural distribution: 2 causes ‚Üí divide by 2\nFinal salience: |I| = 0.75 \\div 2 = 0.375 per cause\n\n(In practice, we round to the nearest conventional value: 0.5)\n\n\n\n\nDefault Assumption\nIf no markers and no competing causes: Assume monocausal attribution (|I| = 1.0)\nThis reflects the discourse convention that unmarked causal statements present causes as primary factors unless explicitly qualified.",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#integration-computing-final-i",
    "href": "processing/tuple-construction.html#integration-computing-final-i",
    "title": "Tuple Construction",
    "section": "Integration: Computing Final I",
    "text": "Integration: Computing Final I\nCombining all components:\n\nI = \\text{sign}(\\text{polarity after negation}) \\times |\\text{salience}|\n\n\nComplete Example\n\n‚ÄúMainly pesticides and habitat loss contribute to bee mortality.‚Äù\n\nStep 1: Entities - Indicator: contribute to (transitive) - C_1 = \\text{pesticides}, C_2 = \\text{habitat loss} - E = \\text{bee mortality}\nStep 2: Polarity - Indicator contribute ‚Üí I_{\\text{default}} &gt; 0 (promoting) - No negation markers - Final polarity: +\nStep 3: Salience - Explicit marker: mainly ‚Üí base = 0.75 - Structural: 2 causes ‚Üí divide by 2 - Salience per cause: |I| = 0.75 \\div 2 \\approx 0.5\nResult: - (C=\\text{pesticides}, E=\\text{bee mortality}, I=+0.5) - (C=\\text{habitat loss}, E=\\text{bee mortality}, I=+0.5)",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "processing/tuple-construction.html#next-steps",
    "href": "processing/tuple-construction.html#next-steps",
    "title": "Tuple Construction",
    "section": "Next Steps",
    "text": "Next Steps\nOnce tuples are constructed, they can be:\n\nAggregated across multiple texts to build evidence for causal relations\nIntegrated into ACGs for graph-based discourse analysis\nAnalyzed for discourse patterns, temporal dynamics, and argumentation structures\n\nThe systematic transformation from qualitative annotations to quantitative tuples bridges interpretive linguistics and computational analysis, enabling scalable yet semantically rich causal extraction.",
    "crumbs": [
      "Processing",
      "Tuple Construction"
    ]
  },
  {
    "objectID": "extraction/c-bert.html",
    "href": "extraction/c-bert.html",
    "title": "C-BERT",
    "section": "",
    "text": "C-BERT [1] is a multi-task transformer to extract fine-grained causal relations as (C, E, I) tuples from text. It classifies tokens as entities and indicators using BIOES [2] tags, then classifies their relationships.\n\nEach indicator ‚Üí entity relation is described in terms of role, polarity and salience ‚Äì either by a unified attention-head or three separate heads.\n\n\n\n\n\n\n\ngraph TB\n    Input[\"Token\"] --&gt; Encoder[\"Relations\"]\n\n\n\n\n\n\n\n\n\nThe factorization is linguistically motivated: an entity‚Äôs role depends on its syntactic position, its polarity on indicator class and negation, and its salience on determiners, coordination, and context markers. Each head can tend on its own type of signal.\nThe German models are built on the EuroBERT family [3], fine-tuned via LoRA [4], using newspaper articles and plenary debates focused on biodiversity topics (see Annotation).\n\n\n\n\n\n\nNoteResources\n\n\n\n\n\n\nModel weights: HuggingFace ‚Äî pdjohn/C-EBERT-610m\nCode: GitHub ‚Äî padjohn/cbert\nData subset: HuggingFace ‚Äî pdjohn/bundestag-causal-attribution (487 relations from German parliamentary debates)\nPaper: Johnson (2025), C-BERT: Factorized Causal Relation Extraction\nAnnotation guidelines: Annotation",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#overview",
    "href": "extraction/c-bert.html#overview",
    "title": "C-BERT",
    "section": "",
    "text": "C-BERT [1] is a multi-task transformer to extract fine-grained causal relations as (C, E, I) tuples from text. It classifies tokens as entities and indicators using BIOES [2] tags, then classifies their relationships.\n\nEach indicator ‚Üí entity relation is described in terms of role, polarity and salience ‚Äì either by a unified attention-head or three separate heads.\n\n\n\n\n\n\n\ngraph TB\n    Input[\"Token\"] --&gt; Encoder[\"Relations\"]\n\n\n\n\n\n\n\n\n\nThe factorization is linguistically motivated: an entity‚Äôs role depends on its syntactic position, its polarity on indicator class and negation, and its salience on determiners, coordination, and context markers. Each head can tend on its own type of signal.\nThe German models are built on the EuroBERT family [3], fine-tuned via LoRA [4], using newspaper articles and plenary debates focused on biodiversity topics (see Annotation).\n\n\n\n\n\n\nNoteResources\n\n\n\n\n\n\nModel weights: HuggingFace ‚Äî pdjohn/C-EBERT-610m\nCode: GitHub ‚Äî padjohn/cbert\nData subset: HuggingFace ‚Äî pdjohn/bundestag-causal-attribution (487 relations from German parliamentary debates)\nPaper: Johnson (2025), C-BERT: Factorized Causal Relation Extraction\nAnnotation guidelines: Annotation",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#architecture",
    "href": "extraction/c-bert.html#architecture",
    "title": "C-BERT",
    "section": "Architecture",
    "text": "Architecture\nC-BERT performs two tasks on a shared encoder:\n\n\n\n\n\ngraph TB\n    Input[\"Input Sentence\"] --&gt; Encoder[\"EuroBERT-610m + LoRA\"]\n\n    Encoder --&gt; T1[\"Task 1: Span Recognition\"]\n    Encoder --&gt; T2[\"Task 2: Relation Classification\"]\n\n    T1 --&gt; BIOES[\"BIOES Tags&lt;br/&gt;(INDICATOR, ENTITY)\"]\n\n    T2 --&gt; Role[\"Role Head&lt;br/&gt;(CAUSE, EFFECT, NO_RELATION)\"]\n    T2 --&gt; Pol[\"Polarity Head&lt;br/&gt;(POS, NEG)\"]\n    T2 --&gt; Sal[\"Salience Head&lt;br/&gt;(MONO, PRIO, DIST)\"]\n\n    BIOES --&gt; Pipeline[\"Tuple Construction\"]\n    Role --&gt; Pipeline\n    Pol --&gt; Pipeline\n    Sal --&gt; Pipeline\n    Pipeline --&gt; Tuples[\"(C, E, I) Tuples\"]\n\n\n\n\n\n\n\nTask 1: Span Recognition\nA token classification head assigns BIOES tags to identify causal indicators and entities in the input sentence:\n\n\n\nTag\nMeaning\n\n\n\n\nB-INDICATOR\nBeginning of a causal indicator span\n\n\nI-INDICATOR\nInside a causal indicator span\n\n\nE-INDICATOR\nEnd of a causal indicator span\n\n\nS-INDICATOR\nSingle-token indicator\n\n\nB-ENTITY\nBeginning of a causal entity span\n\n\nI-ENTITY / E-ENTITY / S-ENTITY\n(analogous)\n\n\nO\nOutside any causal span\n\n\n\n\n\nTask 2: Relation Classification\nFor each (indicator, entity) pair extracted from Task 1, the relation head determines the causal relationship. The input is formatted as:\n[indicator] &lt;|parallel_sep|&gt; [entity] &lt;|parallel_sep|&gt; [sentence]\nThe CLS representation passes through three parallel heads:\nRole (3-class) determines whether the entity is a Cause, Effect, or unrelated to the indicator. This depends primarily on syntactic position and indicator projection patterns.\nPolarity (2-class, masked for NO_RELATION) determines whether the causal influence is promoting (POS) or inhibiting (NEG). This is driven by indicator lexical class and negation context.\nSalience (3-class, masked for NO_RELATION, applied to CAUSE only) determines causal strength:\n\n\n\nClass\n|I|\nMeaning\n\n\n\n\nMONO\n1.0\nMonocausal ‚Äî sole or primary cause\n\n\nPRIO\n0.75\nPrioritized ‚Äî highlighted among multiple factors\n\n\nDIST\n0.5\nDistributed ‚Äî one of several contributing factors\n\n\n\nEffect entities inherit salience from their associated indicator‚Äìcause relation. The final influence value is reconstructed as I = \\text{sign}(\\text{polarity}) \\times s_{\\text{salience}}, or 0 for NO_RELATION.\n\n\nWhy Factorize?\nThe full combinatorial label space has 14 classes: \\{MONO, PRIO, DIST\\} \\times \\{POS, NEG\\} \\times \\{CAUSE, EFFECT\\} = 12, plus NO_RELATION and INTERDEPENDENCY. Flat classification over this space suffers from class sparsity (several classes have fewer than 10 training instances) and conflates signals governed by different linguistic cues.\nFactorization addresses both problems. It reduces per-head complexity (3-class and 2-class instead of 14-class), eliminates class sparsity within each head, and allows each head to learn from its own loss signal. In practice, the factorized model consistently outperforms unified classification across all random seeds tested.\nTwo intermediate architectures were explored and abandoned during development. A role + influence regression head (\\tanh \\to [-1,1]) could not jointly learn sign and magnitude, causing outputs to cluster near zero when negation markers were present. A discrete role/polarity + continuous salience variant defaulted to safe intermediate values (~0.85) rather than learning the categorical distinction between MONO, PRIO, and DIST. Both failures motivated the fully discretized three-head design.",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#training",
    "href": "extraction/c-bert.html#training",
    "title": "C-BERT",
    "section": "Training",
    "text": "Training\n\nData\nThe model is trained on 2,391 manually annotated causal relations from German environmental discourse (1990‚Äì2022), covering four focal terms: Waldsterben (forest dieback), Artensterben (species extinction), Bienensterben (bee death), and Insektensterben (insect death). See Annotation for the full annotation schema and guidelines.\nThe data is split 80/20 at the sentence level (3,802 train / 951 test sentences), with data augmentation (entity replacement) doubling the relation training instances to 7,604. The split is performed before augmentation to prevent leakage.\n\n\nNegation-Aware Target Construction\nA critical preprocessing step separates three distinct negation signals that would otherwise cause the model to learn spurious correlations:\n\nIndicator base polarity ‚Äî looked up from the indicator family taxonomy (e.g.¬†verursachen ‚Üí +, stoppen ‚Üí ‚àí)\nPropositional negation ‚Äî particles like nicht, kein that neutralize the entire relation (these are dropped from training as they are too sparse for the model to learn reliably)\nObject negation ‚Äî negation nominals like Verlust, R√ºckgang in entity spans that invert polarity compositionally: \\text{polarity}_{\\text{final}} = \\text{base} \\times (-1)^{\\text{neg count}}\n\n\n\nHyperparameters\n\n\n\nParameter\nValue\n\n\n\n\nBase model\nEuroBERT-610m\n\n\nLoRA rank / alpha / dropout\n16 / 32 / 0.05\n\n\nLearning rate\n3 \\times 10^{-4} (cosine schedule)\n\n\nWarmup ratio\n0.05\n\n\nEpochs\n7\n\n\nBatch size\n32\n\n\nLoss weights (\\lambda_p, \\lambda_s)\n1.0, 1.0\n\n\nAugmentation\nMode 2 (original + augmented)\n\n\n\nThe total loss is: \\mathcal{L} = \\mathcal{L}_{\\text{role}} + \\lambda_p \\mathcal{L}_{\\text{polarity}} + \\lambda_s \\mathcal{L}_{\\text{salience}}, where all three terms use weighted cross-entropy with inverse-frequency class weights. Polarity and salience losses are masked for NO_RELATION samples.",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#results",
    "href": "extraction/c-bert.html#results",
    "title": "C-BERT",
    "section": "Results",
    "text": "Results\n\nFlagship Comparison (seed 456)\n\n\n\nMetric\nUnified (v2)\nFactorized (v3)\nŒî\n\n\n\n\nRole Accuracy\n‚Äî\n88.7\n\n\n\nPolarity Accuracy\n‚Äî\n92.0\n\n\n\nSalience Accuracy\n‚Äî\n92.4\n\n\n\nReconstructed 14-class Accuracy\n75.3\n76.9\n+1.6\n\n\nReconstructed 14-class F1\n61.9\n62.2\n+0.3\n\n\nTotal errors\n248\n234\n‚àí14\n\n\nMulti-head errors (% of total)\n22.6%\n16.2%\n‚àí6.4\n\n\nEntity F1 (strict span match)\n0.691\n0.765\n+0.074\n\n\nIndicator F1 (strict span match)\n0.649\n0.768\n+0.119\n\n\n\nThe factorized model produces fewer total errors and, critically, a qualitatively different error profile: it reduces multi-head error cascades (where role, polarity, and salience are all wrong simultaneously) from 22.6% to 16.2% of errors, concentrating failures in single, interpretable subtasks.\nAn unexpected finding is that factorization substantially improves span detection despite both architectures sharing the same token classification head ‚Äî suggesting that the factorized relation loss provides gradient signals more compatible with the span detection objective.\n\n\nMulti-Seed Robustness\nAcross five random seeds, the factorized model consistently outperforms the unified model:\n\n\n\n\nUnified (v2)\nFactorized (v3)\n\n\n\n\nMean accuracy\n0.744 \\pm 0.007\n\\mathbf{0.768 \\pm 0.009}\n\n\nBest seed\n0.753\n0.781\n\n\nWorst seed\n0.733\n0.760\n\n\n\nThe factorized model outperforms the unified model on all five seeds tested. Ablation confirms this is a structural advantage ‚Äî scaling the unified model‚Äôs loss to match the factorized model‚Äôs gradient budget does not close the gap.\n\n\nWhat the Model Gets Right\nObject negation without explicit span detection. The model correctly inverts polarity from object negation nominals (e.g.¬†Verlust, Vernichtung) even when these are not detected as separate spans ‚Äî the relation head has learned to attend to negation context in the sentence.\nPassive and non-canonical word order. In Insektensterben wird durch Pestizide verursacht (‚Äúinsect death is caused by pesticides‚Äù), the model correctly assigns roles semantically rather than positionally: Insektensterben (syntactic subject) ‚Üí EFFECT, Pestizide (syntactic oblique) ‚Üí CAUSE.\nExplicit coordination. In Pestizide und Klimawandel verursachen Insektensterben, both causes correctly receive MONO salience. This is by design: both are explicitly named, and salience reduction to DIST/PRIO is reserved for implicit co-causes. Normalization happens downstream during graph aggregation.\n\n\nKnown Limitations\nContext-based salience detection. The model reliably detects salience when it is lexicalized in indicator compounds (Hauptursache ‚Üí PRIO, mitverantwortlich ‚Üí DIST) but struggles when salience is projected by context markers alone: separated verbs (tragen‚Ä¶bei), indefinite determiners (eine Ursache), and priority adverbials (vor allem) often fail to trigger the correct salience class.\nClass imbalance. The relation label distribution is heavily skewed: MONO_POS_EFFECT and MONO_POS_CAUSE together account for 61% of training instances. Rare classes like PRIO_NEG (1 instance) remain difficult despite inverse-frequency class weighting.\nIntra-sentence scope. The model extracts relations within single sentences only. Cross-sentence causality requires discourse parsing, which is left to future work.",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#usage",
    "href": "extraction/c-bert.html#usage",
    "title": "C-BERT",
    "section": "Usage",
    "text": "Usage\n\nInstallation\npip install causalbert\n# or from source:\ngit clone https://github.com/padjohn/cbert\ncd cbert && pip install -e .\n\n\nQuick Start\nfrom causalbert.infer import load_model, sentence_analysis, extract_tuples\n\n# Load model\nmodel, tokenizer, config, device = load_model(\"pdjohn/C-EBERT-610m\")\n\n# Analyze sentences\nsentences = [\n    \"Pestizide verursachen Insektensterben.\",\n    \"Naturschutzma√ünahmen stoppen das Artensterben.\",\n]\n\nresults = sentence_analysis(model, tokenizer, config, sentences, device=device)\n\n# Extract (C, E, I) tuples\ntuples = extract_tuples(results, min_confidence=0.5)\n\nfor t in tuples:\n    print(f\"({t['cause']}, {t['effect']}, {t['influence']:.2f})\")\n    # ‚Üí (Pestizide, Insektensterben, +1.00)\n    # ‚Üí (Naturschutzma√ünahmen, Artensterben, -1.00)\n\n\nPipeline Steps\nThe sentence_analysis function runs the full extraction pipeline:\n\nToken classification ‚Äî predicts BIOES tags for each token\nSpan merging ‚Äî groups tagged tokens into indicator and entity spans\nPair construction ‚Äî creates all (indicator, entity) combinations\nRelation classification ‚Äî predicts role, polarity, and salience for each pair\nTuple extraction ‚Äî extract_tuples() converts results to (C, E, I) dictionaries\n\nEach tuple contains: cause, effect, influence (\\in [-1, +1]), sentence, confidence, and label.\n\n\nInference Performance\nWith LoRA fine-tuning, only 0.6M additional parameters are trained on top of the 610M base model. End-to-end inference (span detection + relation classification) takes approximately 37 ms per sentence on an NVIDIA RTX 4090 (batch size 1). The factorized heads add minimal overhead compared to flat classification.\nAt batch size 32, the full environmental corpus of 22 million sentences was processed in approximately 10 hours on GPU, yielding 1.6 million unique aggregated causal relations across 357,000 distinct entities.",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#model-variants",
    "href": "extraction/c-bert.html#model-variants",
    "title": "C-BERT",
    "section": "Model Variants",
    "text": "Model Variants\nBoth architectures are released:\n\n\n\n\n\n\n\n\nVariant\nDescription\nUse case\n\n\n\n\nv3 (factorized)\nThree parallel heads (role, polarity, salience)\nRecommended default ‚Äî better accuracy, interpretable errors\n\n\nv2 (unified)\nSingle 14-class softmax\nSimpler pipeline, single prediction per pair\n\n\n\nThe architecture version is stored in the model config and automatically detected at load time.",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/c-bert.html#further-reading",
    "href": "extraction/c-bert.html#further-reading",
    "title": "C-BERT",
    "section": "Further Reading",
    "text": "Further Reading\n\nFor the annotation schema and guidelines that produced the training data, see Annotation\nFor how extracted tuples are transformed into formal (C, E, I) values, see Tuple Construction\nFor a high-level view of the extraction pipeline, see Extraction Overview\nFor the theoretical framework motivating polarity and salience, see Framework",
    "crumbs": [
      "Extraction",
      "C-BERT"
    ]
  },
  {
    "objectID": "extraction/extraction.html",
    "href": "extraction/extraction.html",
    "title": "Extraction Overview",
    "section": "",
    "text": "Extracting causal relations requires at least two components:\n\nIndicators: operators projecting causal roles\n\ne.g.¬†cause, contribute, reduce, prevent\n\nEntities: arguments that function as Cause and/or Effect\n\ne.g.¬†climate change, emission, poverty, war\n\n\nIndicators can be classified as polar positive (e.g.¬†cause) and negative (e.g.¬†prevent) [1] [2]. Following [3], S_C further distinguishes between mono- (e.g.¬†prevent) and polycausal (e.g reduce) relationships. Both dimensions can be modified by contextual markers:\n\nPolarity: death of, rise in\nSalience: less so, especially\n\nCausal Relation Extraction (CRE) has to identify these components ‚Äì classify them in terms of polarity and salience ‚Äì and apply those values to variable syntactic scopes.\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nIdentification and classification of indicators:\n\nAbsence of emissions hinders climate change\nhinder = polycausal-negative (I = -0.5)\n(C, E, -0.5)\n\nIdentification of Cause and Effect entities:\n\nAbsence of emissions hinders climate change\nCause = Subject = emissions, Effect = Direct Object = climate change\n(\\text{Emission}, \\text{Climate change}, 0.5)\n\nIdentification, classification and scope of coefficient markers:\n\nAbsence of emissions hinders climate change\nAbsence of = negates emissions (-1)\n(\\text{Emission}, \\text{Climate change}, 0.5)\n\n(C, E, I) = (\\text{Emission}, \\text{Climate change}, 0.5)\nFor more examples, see Tuple Construction.",
    "crumbs": [
      "Extraction",
      "Extraction Overview"
    ]
  },
  {
    "objectID": "extraction/extraction.html#components",
    "href": "extraction/extraction.html#components",
    "title": "Extraction Overview",
    "section": "",
    "text": "Extracting causal relations requires at least two components:\n\nIndicators: operators projecting causal roles\n\ne.g.¬†cause, contribute, reduce, prevent\n\nEntities: arguments that function as Cause and/or Effect\n\ne.g.¬†climate change, emission, poverty, war\n\n\nIndicators can be classified as polar positive (e.g.¬†cause) and negative (e.g.¬†prevent) [1] [2]. Following [3], S_C further distinguishes between mono- (e.g.¬†prevent) and polycausal (e.g reduce) relationships. Both dimensions can be modified by contextual markers:\n\nPolarity: death of, rise in\nSalience: less so, especially\n\nCausal Relation Extraction (CRE) has to identify these components ‚Äì classify them in terms of polarity and salience ‚Äì and apply those values to variable syntactic scopes.\n\n\n\n\n\n\nNoteExample\n\n\n\n\n\nIdentification and classification of indicators:\n\nAbsence of emissions hinders climate change\nhinder = polycausal-negative (I = -0.5)\n(C, E, -0.5)\n\nIdentification of Cause and Effect entities:\n\nAbsence of emissions hinders climate change\nCause = Subject = emissions, Effect = Direct Object = climate change\n(\\text{Emission}, \\text{Climate change}, 0.5)\n\nIdentification, classification and scope of coefficient markers:\n\nAbsence of emissions hinders climate change\nAbsence of = negates emissions (-1)\n(\\text{Emission}, \\text{Climate change}, 0.5)\n\n(C, E, I) = (\\text{Emission}, \\text{Climate change}, 0.5)\nFor more examples, see Tuple Construction.",
    "crumbs": [
      "Extraction",
      "Extraction Overview"
    ]
  },
  {
    "objectID": "extraction/extraction.html#application",
    "href": "extraction/extraction.html#application",
    "title": "Extraction Overview",
    "section": "Application",
    "text": "Application\nTransforming text into tuples can be achieved in a variety of manual or automatic ways.\nBoth rule-based [4] and prompt-based [5] approaches have been applied to CRE, though neither incorporate salience nor polarity. As of today, a mixture of manual annotation and transformers [6] appear the most promising.\nThe following sections provide a brief overview of this two-path structure ‚Äì combining the scalability and determinism of an encoder-only transformer with the interpretability of a manually annotated dataset.\n\nAnnotation\nThe annotation schema consists of span annotations (indicators, entities, and semantic coefficients like negation and division) ‚Äì linked by directed relations (Cause, Effect, Constraint).\nA taxonomy of 642 indicator forms organized into 192 families provides the linguistic foundation: each indicator carries an inherent polarity and salience. As presented above, these values are further modified by context markers (division, priority, negation).\nThe annotation guidelines serve as a reference for manual annotation. At the same time, the annotation also produces the training data for C-BERT [7].\n‚Üí Annotation Guidelines: Full schema, annotation principles, indicator taxonomy, context markers, INFLUENCE computation, and data format\n\n\nC-BERT\nC-BERT is a multi-task transformer built on EuroBERT-610m [8]. It emulates manual annotation through span recognition and relation classification.\nThe pipeline proceeds in three steps:\n\nSpan classification predicts BIOES tags for each token.\n\nINDICATOR, ENTITY, O\n\nPair construction constructs indicator/entity pairs from extracted spans.\n\n[INDICATOR_1, ENTITY_1], ...\\;,[INDICATOR_n, ENTITY_n]\n\nRelation classification determines, for each pair, the projected\n\nrole (CAUSE, EFFECT, NO_RELATION)\npolarity (POS, NEG)\nsalience (MONO, DIST, PRIO)\n\n\nThe classified [INDICATOR, ENTITY] relationships are then algorithmically collapsed into (C, E, I)-tuples (see Tuple Construction).\n\n\n\n\n\ngraph LR\n    A[Text] --&gt; B2[Span&lt;br/&gt;classification]\n    B2 --&gt; D[Pair&lt;br/&gt;construction]\n    D --&gt; E[Relation&lt;br/&gt;classification]\n    E --&gt; F[\"$$(C, E, I)\\;$$\"]\n\n\n\n\n\n\n‚Üí C-BERT Model: Architecture, training, results, known limitations, and usage instructions",
    "crumbs": [
      "Extraction",
      "Extraction Overview"
    ]
  },
  {
    "objectID": "extraction/extraction.html#at-a-glance",
    "href": "extraction/extraction.html#at-a-glance",
    "title": "Extraction Overview",
    "section": "At a Glance",
    "text": "At a Glance\n\n\n\n\n\n\n\nTraining data\n2,391 relations across 4,753 sentences (German environmental discourse)\n\n\nIndicator taxonomy\n642 forms in 192 families, each classified by polarity and salience\n\n\nModel\nEuroBERT-610m + LoRA, factorized 3-head relation classification\n\n\nPer-head accuracy\nRole: 88.7%, Polarity: 92.0%, Salience: 92.4%\n\n\nReconstructed accuracy\n76.9% (14-class)\n\n\nSpan detection\nEntity F1: 0.765, Indicator F1: 0.768\n\n\nInference speed\n~37 ms/sentence (RTX 4090, batch size 1)\n\n\nCorpus-scale output\n22M sentences ‚Üí 1.6M unique relations, 357K entities",
    "crumbs": [
      "Extraction",
      "Extraction Overview"
    ]
  },
  {
    "objectID": "extraction/extraction.html#continue",
    "href": "extraction/extraction.html#continue",
    "title": "Extraction Overview",
    "section": "Continue",
    "text": "Continue\n\nAnnotation Guidelines ‚Äî the schema, principles, and data format behind the training data\nC-BERT Model ‚Äî architecture, experiments, and how to use the model\nTuple Construction ‚Äî how annotations become formal (C, E, I) values",
    "crumbs": [
      "Extraction",
      "Extraction Overview"
    ]
  }
]